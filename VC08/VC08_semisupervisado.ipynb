{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>VC08: Aprendizaje semi-supervisado</h1></center>\n",
    "\n",
    "En esta práctica aprenderemos a llevar a cabo un aprendizaje semi-supervisado. En concreto, estudiaremos el funcionamiento y la utilización del popular algoritmo de EM para aprender un modelo naive Bayes en semi-supervisado, así como la estrategia de aprendizaje conocida como \"co-training\".\n",
    "\n",
    "Empezaremos por programar una implementación del clasificador que usaremos, el clásico Naive Bayes. Este clasificador probabilístico guarda los parámetros relativos a la distribución de probabilidad marginal de clase ($\\{p(C=c)\\}_{c=1}^{|C|}$) y las distribuciones de probabilidad condicional de las variables predictoras dada la clase ($\\{p(X_i=x_i|C=c)\\}_{x_i=1}^{|X_i|}$ para los diferentes valores de la variable clase $C=c$ y para todas las variables predictoras). Con estos parámetros se puede calcular la probabilidad de un caso, $\\mathbf{x}$:\n",
    "$$  p(C=c|\\mathbf{x})\\propto p(C=c)\\prod_{i=1}^{v} p(x_i|C=c)$$\n",
    "normalizándolos para todos los posibles valores de $c$ (todas las clases posibles) y \n",
    "con $v$ siendo el número de variables predictoras \n",
    "(2 en este caso) para que la \n",
    "suma sea $\\sum_{c=1}^{|C|} p(C=c|\\mathbf{x})=1$.\n",
    "\n",
    "Por conveniencia, también guardaremos otras variables como la cardinalidad de las diferentes variables y el índice de la variable clase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class naiveBayes:\n",
    "    def __init__(self, iClass, cardinalities):\n",
    "        self.iClass = iClass\n",
    "        self.cardinalities = cardinalities.copy()\n",
    "        self.Pc = np.zeros(cardinalities[iClass])\n",
    "        self.Pxc = []\n",
    "        for i in np.arange(len(self.cardinalities)):\n",
    "            aux = np.array([])\n",
    "            if i != iClass:\n",
    "                aux = np.zeros((cardinalities[i],cardinalities[iClass]))\n",
    "            self.Pxc.append(aux)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez diseñado, vamos a lo importante. Un clasificador ha de \n",
    "poder ser aprendido y usado para predecir. Por ello, diseñaremos \n",
    "una función para cada procedimiento. \n",
    "Para predecir, se han de calcular las siguiente probabilidades:\n",
    "$$ p(C=c)\\prod_{i=1}^{v} p(x_i|C=c)$$\n",
    "para todos los posibles valores de $c$ (todas las clases posibles) y \n",
    "con $v$ siendo el número de variables predictoras \n",
    "(2 en este caso). Finalmente, normalizamos para que la \n",
    "suma de $\\sum_{c=1}^{|C|} p(C=c|x)=1$.\n",
    "La función de predicción \n",
    "sería así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictNB(model, instance):\n",
    "    probs = model.Pc.copy()\n",
    "    for i in np.arange(len(model.cardinalities)):\n",
    "        if i != model.iClass:\n",
    "            probs *= model.Pxc[i][#### TU CODIGO AQUI ####]\n",
    "\n",
    "    return probs/np.sum(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Considerando la estructura definida para guardar los \n",
    "parámetros y lo visto en clase sobre aprendizaje de \n",
    "parámetros de máxima verosimilitud en un entorno de \n",
    "aprendizaje semi-supervisado, diseñaremos la función \n",
    "de aprendizaje:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnNB(L, U, pesosU, iClass, cardinalities, smoothing=1):\n",
    "    modelo = naiveBayes(iClass, cardinalities)\n",
    "    \n",
    "    # Aprender de casos etiquetados\n",
    "    for i in np.arange(L.shape[0]):\n",
    "        modelo.Pc[L[i,iClass]] += 1\n",
    "        \n",
    "        for j in np.arange(len(cardinalities)):\n",
    "            if j != iClass:\n",
    "                modelo.Pxc[j][L[i,j],L[i,iClass]] += #### TU CODIGO AQUI ####\n",
    "\n",
    "    for u in np.arange(U.shape[0]):\n",
    "        modelo.Pc += pesosU[u,:]\n",
    "        \n",
    "        for j in np.arange(len(cardinalities)):\n",
    "            if j != iClass:\n",
    "                modelo.Pxc[j][U[u,j],:] += #### TU CODIGO AQUI ####\n",
    "    \n",
    "    modelo.Pc += smoothing # Laplace smoothing\n",
    "    modelo.Pc /= np.sum(modelo.Pc)\n",
    "    for j in np.arange(len(cardinalities)):\n",
    "        if j != iClass:\n",
    "            modelo.Pxc[j] += smoothing # Laplace smoothing\n",
    "            modelo.Pxc[j] /= np.sum(modelo.Pxc[j],0)\n",
    "    \n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Con esto completamos el diseño del clasificador Naive Bayes para el entorno semi-supervisado.\n",
    "\n",
    "Ahora ya podemos crear el algoritmo EM, que itera los pasos E y M de forma sencilla hasta que converge:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM(L,U, iClass, cardinalities, epsilon=0.001):\n",
    "    modelo, pesosU = inicializar(L, U, iClass, cardinalities)\n",
    "\n",
    "    convergencia = False\n",
    "    it = 0\n",
    "    while not convergencia:\n",
    "        it += 1\n",
    "        print('Iteracion', it)\n",
    "        pesosU = EStep(L, U, pesosU, modelo)\n",
    "\n",
    "        antModelo = modelo\n",
    "        \n",
    "        modelo = MStep(L, U, pesosU, iClass, cardinalities, antModelo)\n",
    "    \n",
    "        convergencia = testConvergencia(modelo, antModelo, epsilon)\n",
    "    \n",
    "    return modelo, pesosU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faltarían por programar las tres funciones fundamentales \n",
    "del EM: el paso E (<b>EStep</b>),  el paso M (<b>MStep</b>) \n",
    "y la función que comprueba la convergencia (<b>testConvergencia</b>). \n",
    "Además, falta la inicialización donde asignamos valores iniciales \n",
    "a los pesos de las instancias no etiquetadas para aprender la \n",
    "primera versión del modelo. En concreto, en esta ocasión \n",
    "asignaremos a todos los casos la misma probabilidad de \n",
    "pertenecer a cualquier clase:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializar(L, U, iClass, cardinalities):\n",
    "    pesosU = np.ones((U.shape[0],cardinalities[iClass]))\n",
    "    pesosU /= cardinalities[iClass]\n",
    "    \n",
    "    modelo = learnNB(L, U, pesosU, iClass, cardinalities)\n",
    "\n",
    "    return modelo, pesosU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El paso E consiste en recalcular los pesos dado un modelo y el paso M en aprender una nueva versión del modelo:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EStep(L, U, pesosU, modelo):\n",
    "    nPesosU = np.zeros(pesosU.shape)\n",
    "    for u in np.arange(U.shape[0]):\n",
    "        nPesosU[u,:] = #### TU CODIGO AQUI ####\n",
    "\n",
    "    return nPesosU\n",
    "\n",
    "def MStep(L, U, pesosU, iClass, cardinalities, antModelo):\n",
    "    return #### TU CODIGO AQUI ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Mediremos la convergencia atendiendo al criterio de si \n",
    "la distancia euclídea entre los parámetros del paso anterior \n",
    "y del actual es menor que <i>epsilon</i> o no:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testConvergencia(modeloA, modeloB, epsilon=0.001):\n",
    "    resultado = np.sum((modeloA.Pc-modeloB.Pc)**2)\n",
    "\n",
    "    for j in np.arange(len(modeloA.cardinalities)):\n",
    "        if j != modeloA.iClass:\n",
    "            resultado += np.sum((modeloA.Pxc[j]-modeloB.Pxc[j])**2)\n",
    "  \n",
    "    resultado = #### TU CODIGO AQUI ####\n",
    "    \n",
    "    return resultado < epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Una vez hemos diseñado completamente el algoritmo EM y el clasificador NB, podemos proceder a su uso. \n",
    "Creamos un dataset y hacemos la llamada al EM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "np.random.seed(23)\n",
    "nPredVars = 10\n",
    "iClass = nPredVars\n",
    "nSampleXsubset = 20\n",
    "cardinalities = np.repeat(2,nPredVars+1)\n",
    "\n",
    "# Simulamos un dataset\n",
    "X,y = make_classification(n_samples=nSampleXsubset*3,n_features=nPredVars,n_redundant=0)\n",
    "X[X<0] = 0;X[X>0] = 1 # discretizamos las variables predictoras\n",
    "X = X.astype(int)\n",
    "y.shape=(len(y),1)\n",
    "\n",
    "L = np.concatenate((X[:nSampleXsubset,:],\n",
    "                    y[:nSampleXsubset,:]),axis=1)\n",
    "U = np.concatenate((X[nSampleXsubset:(nSampleXsubset*2),:],\n",
    "                    y[nSampleXsubset:(nSampleXsubset*2),:]),axis=1)\n",
    "U[:,iClass] = np.nan\n",
    "\n",
    "modelo, pesos = EM(L, U, iClass, cardinalities, 0.001)\n",
    "print('Peso: ',pesos[1,0], ' (** ESTE ES EL RESULTADO A INCLUIR EN EL CAMPUS**)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Como en anteriores ocasiones, podemos estudiar la bondad del agrupamiento ya que se conoce la realidad:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.concatenate((X[nSampleXsubset*2:,:],\n",
    "                    y[nSampleXsubset*2:,:]),axis=1)\n",
    "\n",
    "realLabels = test[:,iClass]\n",
    "predLabels = np.zeros(realLabels.shape)\n",
    "for i in np.arange(test.shape[0]):\n",
    "    probs = predictNB(modelo, test[i,:])\n",
    "    predLabels[i] = np.argmax(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matriz_confusion(cat_real, cat_pred):\n",
    "    cats = np.unique(cat_real)\n",
    "    clusts = np.unique(cat_pred)\n",
    "    mat = np.array([[np.sum(np.logical_and(cat_real==cats[i], cat_pred==clusts[j])) \n",
    "                     for j in np.arange(clusts.size)] \n",
    "                    for i in np.arange(cats.size)])\n",
    "    return(mat)\n",
    "\n",
    "def medida_error(mat):\n",
    "    tot = np.sum(mat)\n",
    "    aux = mat.copy()\n",
    "    np.fill_diagonal(aux, 0)\n",
    "    return float(np.sum(aux))/tot\n",
    "\n",
    "def medida_precision(mat, l, k):\n",
    "    return mat[l,k]/float(np.sum(mat[:,k]))\n",
    "\n",
    "def medida_recall(mat, l, k):\n",
    "    return mat[l,k]/float(np.sum(mat[l,:]))\n",
    "\n",
    "def medida_f1_especifica(mat, l, k):\n",
    "    prec = medida_precision(mat, l, k)\n",
    "    rec = medida_recall(mat, l, k)\n",
    "    if (prec+rec)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2*prec*rec/(prec+rec)\n",
    "\n",
    "def medida_f1(mat):\n",
    "    return medida_f1_especifica(mat, 1, 1)\n",
    "\n",
    "mC = matriz_confusion(realLabels,predLabels)\n",
    "\n",
    "print(mC)\n",
    "print('El error del clasificador es = ', medida_error(mC))\n",
    "print('El valor F1 del clasificador es = ', medida_f1(mC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h2>Implementaciones en librerías de Python</h2>\n",
    "\n",
    "La librería ScikitLearn implementa diversas versiones del clasificador NB. En el caso de <b>MultinomialNB</b>, la distribución de probabilidad que se modela es diferente: una distribución de probabilidad multinomial donde el valor de cada variable predictora es el conteo de esta distribución de probabilidad:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\Pr(X_{1}=x_{1},\\dots, X_{v}=x_{v})&{}={\\begin{cases}{\\displaystyle {n! \\over x_{1}!\\cdot\\dots\\cdot x_{k}!}p_{1}^{x_{1}}\\cdot \\dots \\cdot p_{v}^{x_{v}}},\\quad &{\\text{when }}\\sum _{i=1}^{k}x_{i}=n\\\\\\\\0&{\\text{otherwise,}}\\end{cases}}\n",
    "\\end{aligned}\n",
    "para cualquier conjunto $\\{x_1, \\dots, x_v\\}$ de valores no negativos.\n",
    "\n",
    "Si quisiésemos incluir esta versión del NB en el algoritmo EM, deberíamos modificar las funciones específicas que hacen llamadas al modelo pero no la general iterativa del EM.\n",
    "\n",
    "En primer lugar, deberíamos preparar una función para el aprendizaje del modelo cuando los datos son semi-supervisados:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def sklearnNB(L, U, pesosU, iClass, cardinalities):\n",
    "    modelo = MultinomialNB()\n",
    "\n",
    "    modelo.partial_fit(np.delete(L, iClass, axis=1),\n",
    "                       L[:,iClass],\n",
    "                       classes=[0,1])\n",
    "\n",
    "    for c in np.arange(cardinalities[iClass]):\n",
    "        impU = U.copy()\n",
    "        impU[:,iClass] = c\n",
    "        modelo.partial_fit(np.delete(impU, iClass, axis=1),\n",
    "                           impU[:,iClass],\n",
    "                           sample_weight = pesosU[:,c])\n",
    "\n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "La función de predicción del NB, en este caso, ya está integrada en la definición de la clase de Scikit-learn.\n",
    "\n",
    "Las cuatro funciones auxiliares del EM se deben adecuar a este nuevo modelo: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializar(L, U, iClass, cardinalities):\n",
    "    pesosU = np.ones((U.shape[0],cardinalities[iClass]))\n",
    "    pesosU /= cardinalities[iClass]\n",
    "    \n",
    "    modelo = sklearnNB(L, U, pesosU, iClass, cardinalities)\n",
    "\n",
    "    return modelo, pesosU\n",
    "\n",
    "def EStep(L, U, pesosU, modelo):\n",
    "\n",
    "    nPesosU = modelo.predict_proba(np.delete(U, iClass, axis=1))\n",
    "\n",
    "    return nPesosU\n",
    "\n",
    "def MStep(L, U, pesosU, iClass, cardinalities, antModelo):\n",
    "    modelo = sklearnNB(L, U, pesosU, iClass, cardinalities)\n",
    "    return modelo\n",
    "\n",
    "def testConvergencia(modeloA, modeloB, epsilon=0.001):\n",
    "    resultado = np.sum((np.exp(modeloA.class_log_prior_) - \n",
    "                        np.exp(modeloB.class_log_prior_))**2)\n",
    "    resultado += np.sum((np.exp(modeloA.feature_log_prob_) - \n",
    "                         np.exp(modeloB.feature_log_prob_))**2)\n",
    "\n",
    "    resultado = np.sqrt(resultado)\n",
    "    \n",
    "    return resultado < epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Una vez hemos diseñado completamente las distintas funciones del algoritmo EM y esta nueva versión del NB, podemos proceder a su uso. \n",
    "Creamos un dataset y hacemos la llamada al EM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(23)\n",
    "nPredVars = 10\n",
    "iClass = nPredVars\n",
    "nSampleXsubset = 20\n",
    "cardinalities = np.repeat(2,nPredVars+1)\n",
    "\n",
    "# Simulamos un dataset\n",
    "X,y = make_classification(n_samples=nSampleXsubset*3,n_features=nPredVars,n_redundant=0)\n",
    "X[X<0] = 0;X[X>0] = 1 # discretizamos las variables predictoras\n",
    "X = X.astype(int)\n",
    "y.shape=(len(y),1)\n",
    "\n",
    "L = np.concatenate((X[:nSampleXsubset,:],\n",
    "                    y[:nSampleXsubset,:]),axis=1)\n",
    "U = np.concatenate((X[nSampleXsubset:(nSampleXsubset*2),:],\n",
    "                    y[nSampleXsubset:(nSampleXsubset*2),:]),axis=1)\n",
    "U[:,iClass] = np.nan\n",
    "\n",
    "skmodelo, skpesos = EM(L, U, iClass, cardinalities, 0.001)\n",
    "print(skpesos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.concatenate((X[nSampleXsubset*2:,:],\n",
    "                       y[nSampleXsubset*2:,:]),axis=1)\n",
    "\n",
    "skRealLabels = test[:,iClass]\n",
    "skPredLabels = np.zeros(realLabels.shape)\n",
    "test = np.delete(test,iClass,1)\n",
    "skPredLabels = np.argmax(skmodelo.predict_proba(test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mC = matriz_confusion(skRealLabels,skPredLabels)\n",
    "\n",
    "print(mC)\n",
    "print('El error del clasificador es = ', medida_error(mC))\n",
    "print('El valor F1 del clasificador es = ', medida_f1(mC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El algoritmo EM, ejecutado en una única ocasión, puede dar un resultado no óptimo: se está eligiendo una inicialización que lleva al algoritmo a encallar en un óptimo local. Si se ejecuta en varias ocaciones, eventualmente se obtendrá el resultado óptimo.\n",
    "\n",
    "Podríamos comparar el resultado de nuestro algoritmo y el de la implementación de ScikitLearn para observar si devuelven el mismo resultado:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si comparamos el resultado de ambos algoritmos, el nuestro y el de ScikitLearn\n",
    "mC_comp = matriz_confusion(predLabels,skPredLabels)\n",
    "\n",
    "print('Matriz de confusión:')\n",
    "print(mC_comp)\n",
    "print('El valor del error (diferencia) es = ', medida_error(mC_comp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "<center><h1>Co-agrupamiento</h1></center>\n",
    "\n",
    "Si disponemos de dos conjuntos de variables predictoras (disjuntos y, en la medida de lo posible, independientes), se puede alternar el aprendizaje semi-supervisado de uno y otro subconjunto con el objetivo de localizar, mediante uno u otro procedimiento, de algún caso con alta probabilidad de pertenecer a cierta clase. Esta idea la podemos expresar de la siguiente manera. \n",
    "\n",
    "En primer lugar, generaremos los dos conjuntos de variables predictoras:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23)\n",
    "nPredVars = 20\n",
    "iClass = nPredVars\n",
    "nSampleXsubset = 20\n",
    "cardinalities = np.repeat(2,nPredVars+1)\n",
    "\n",
    "\n",
    "# Simulamos un dataset\n",
    "X,y = make_classification(n_samples=nSampleXsubset*2,n_features=nPredVars)\n",
    "X[X<0] = 0;X[X>0] = 1\n",
    "X = X.astype(int)\n",
    "y.shape=(len(y),1)\n",
    "\n",
    "print(X[np.ix_(np.arange(nSampleXsubset),np.arange(nPredVars//2,nPredVars))].shape)\n",
    "print((y[np.arange(nSampleXsubset)].T).shape)\n",
    "L_a = np.concatenate((X[np.ix_(np.arange(nSampleXsubset),np.arange(nPredVars//2,nPredVars))],\n",
    "                      y[np.arange(nSampleXsubset),:]),axis=1)\n",
    "L_b = np.concatenate((X[np.ix_(np.arange(nSampleXsubset),np.arange(nPredVars//2))],\n",
    "                      y[np.arange(nSampleXsubset)]),axis=1)\n",
    "U_a = np.concatenate((X[np.ix_(np.arange(nSampleXsubset,2*nSampleXsubset),np.arange(nPredVars//2,nPredVars))],\n",
    "                      y[np.arange(nSampleXsubset,2*nSampleXsubset),:]),axis=1)\n",
    "U_b = np.concatenate((X[np.ix_(np.arange(nSampleXsubset,2*nSampleXsubset),np.arange(nPredVars//2))],\n",
    "                      y[np.arange(nSampleXsubset,2*nSampleXsubset),:]),axis=1)\n",
    "\n",
    "cardinalities_a = cardinalities[np.arange(nPredVars//2,nPredVars)]\n",
    "cardinalities_b = cardinalities[np.arange(nPredVars//2)]\n",
    "iClass = nPredVars//2\n",
    "U_a[:,iClass] = np.nan\n",
    "U_b[:,iClass] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Así, los casos etiquetados y los no etiquetados están expresados por medio de los dos conjuntos de variables.\n",
    "\n",
    "Si fijamos un umbral <it>th</it> de ejemplos que consideraremos etiquetados si superan ese umbral de probabilidad, el proceso consiste en iterar los dos siguientes subprocesos:\n",
    "\n",
    "- Aprender con las variables del conjunto $A$, localizar posibles ejemplos (positivos y negativos) con alta probabilidad y, en su caso, pasarlos al conjunto de etiquetados.\n",
    "\n",
    "- Aprender con las variables del conjunto $B$, localizar posibles ejemplos (positivos y negativos) con alta probabilidad y, en su caso, pasarlos al conjunto de etiquetados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "th = 0.95\n",
    "\n",
    "seguir = True\n",
    "while seguir :\n",
    "    ini = L_a.shape[0]\n",
    "    modelo_a, pesos_a = EM(L_a, U_a, iClass, cardinalities, 0.001)\n",
    "    print(pesos_a)\n",
    "    toLabelNeg = np.where(pesos_a[:,0]>th)[0]\n",
    "    if len(toLabelNeg)>0:\n",
    "        print('Asignar a negativos: ', toLabelNeg)\n",
    "        aux = U_a[toLabelNeg,:]\n",
    "        aux[:,iClass] = 0\n",
    "        L_a = np.concatenate((L_a,aux),axis=0)\n",
    "        aux = U_b[toLabelNeg,:]\n",
    "        aux[:,iClass] = 0\n",
    "        L_b = np.concatenate((L_b,aux),axis=0)\n",
    "        U_a = np.delete(U_a, toLabelNeg, axis=0)\n",
    "        U_b = np.delete(U_b, toLabelNeg, axis=0)\n",
    "        pesos_a = np.delete(pesos_a, toLabelNeg, axis=0)\n",
    "\n",
    "    toLabelPos = np.where(pesos_a[:,1]>th)[0]\n",
    "    if len(toLabelPos)>0:\n",
    "        print('Asignar a positivos: ', toLabelPos)\n",
    "        aux = U_a[toLabelPos,:]\n",
    "        aux[:,iClass] = 1\n",
    "        L_a = np.concatenate((L_a,aux),axis=0)\n",
    "        aux = U_b[toLabelPos,:]\n",
    "        aux[:,iClass] = 1\n",
    "        L_b = np.concatenate((L_b,aux),axis=0)\n",
    "        U_a = np.delete(U_a, toLabelPos, axis=0)\n",
    "        U_b = np.delete(U_b, toLabelPos, axis=0)\n",
    "        pesos_a = np.delete(pesos_a, toLabelPos, axis=0)\n",
    "\n",
    "    modelo_b, pesos_b = EM(L_b, U_b, iClass, cardinalities, 0.001)\n",
    "    print(pesos_b)\n",
    "\n",
    "    toLabelNeg = np.where(pesos_b[:,0]>th)[0]\n",
    "    if len(toLabelNeg)>0:\n",
    "        print('Asignar a negativos: ', toLabelNeg)\n",
    "        aux = U_a[toLabelNeg,:]\n",
    "        aux[:,iClass] = 0\n",
    "        L_a = np.concatenate((L_a,aux),axis=0)\n",
    "        aux = U_b[toLabelNeg,:]\n",
    "        aux[:,iClass] = 0\n",
    "        L_b = np.concatenate((L_b,aux),axis=0)\n",
    "        U_a = np.delete(U_a, toLabelNeg, axis=0)\n",
    "        U_b = np.delete(U_b, toLabelNeg, axis=0)\n",
    "        pesos_b = np.delete(pesos_b, toLabelNeg, axis=0)\n",
    "\n",
    "    toLabelPos = np.where(pesos_b[:,1]>th)[0]\n",
    "    if len(toLabelPos)>0:\n",
    "        print('Asignar a positivos: ', toLabelPos)\n",
    "        aux = U_a[toLabelPos,:]\n",
    "        aux[:,iClass] = 1\n",
    "        L_a = np.concatenate((L_a,aux),axis=0)\n",
    "        aux = U_b[toLabelPos,:]\n",
    "        aux[:,iClass] = 1\n",
    "        L_b = np.concatenate((L_b,aux),axis=0)\n",
    "        U_a = np.delete(U_a, toLabelPos, axis=0)\n",
    "        U_b = np.delete(U_b, toLabelPos, axis=0)\n",
    "        pesos_b = np.delete(pesos_b, toLabelPos, axis=0)\n",
    "\n",
    "    seguir = (L_a.shape[0] - ini) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para finalizar, y como un simple ejercicio de curiosidad, podemos ver cuántos ejemplos fueron considerados en algún momento del proceso como certero (o con suficiente certeza) y pasaron al grupo de etiquetados:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L_a.shape[0]-nSampleXsubset, 'instancias se incluyeron en el dataset de etiquetados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
