{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VC01_1_distancias.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "OCawOQoQYcDn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<center><h1>VC01: Uso de las medidas de (di)similitud</h1></center>\n",
        "\n",
        "Esta primera práctica está enfocada al estudio de las diferentes medidas de similitud y disimilitud-distancia que hemos visto en la parte teórica. \n",
        "Programaremos nuestras propias versiones de diferentes medidas y veremos cómo se usan las que incorporan las diferentes librerías de Python.\n",
        "\n",
        "Para empezar, cargamos las librerías que vamos a necesitar:\n"
      ]
    },
    {
      "metadata": {
        "id": "kaN6qLV5YcDr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import itertools as it\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FIBxaHW4YcDx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Creamos y cargamos los ejemplos y los conjuntos de datos que usamos en esta práctica:\n"
      ]
    },
    {
      "metadata": {
        "id": "Es6EglSIYcDy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(19) # Fijamos una semilla para asegurar la reproducibilidad de la práctica\n",
        "\n",
        "n_samples = 100\n",
        "C = np.array([[1., 0., -0.1], [1.7, .4, .3], [.7, 1.1, .5]])\n",
        "X = np.dot(np.random.randn(n_samples, 3), C)\n",
        "\n",
        "# Veamos qué pinta tienen los datos\n",
        "fig = plt.figure()\n",
        "ax = Axes3D(fig)\n",
        "ax.scatter(X[2:,0], X[2:,1], X[2:,2])\n",
        "\n",
        "ax.scatter(X[1,0], X[1,1], X[1,2], color='red')\n",
        "ax.text(X[1,0],X[1,1],X[1,2],  '%s' % (\"X-1\"), size=20, zorder=1, color='k') \n",
        "\n",
        "ax.scatter(X[2,0], X[2,1], X[2,2], color='red')\n",
        "ax.text(X[2,0],X[2,1],X[2,2],  '%s' % (\"X-2\"), size=20, zorder=1, color='k') \n",
        "\n",
        "# Cargamos también un conjunto de datos\n",
        "data_file_url = 'https://raw.githubusercontent.com/jhernandezgonzalez/unsupervisedlearning/master/datasets/sinteticos/dataset_dos_guassianas.csv'\n",
        "D = np.array(pd.read_csv(data_file_url))\n",
        "D = D[ np.random.choice(np.arange(D.shape[0]), D.shape[0], replace=False) ,:]\n",
        "Dx = D[D[:,2]==2,0:2] # nos quedamos con uno de los clústeres y eliminamos la variable de la categoría\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(Dx[:,0],Dx[:,1])\n",
        "\n",
        "ax.scatter(Dx[0,0], Dx[0,1], color='red')\n",
        "ax.text(Dx[0,0],Dx[0,1], '%s' % (\"X-0\"), size=20, zorder=1, color='k') \n",
        "ax.scatter(Dx[1,0], Dx[1,1], color='red')\n",
        "ax.text(Dx[1,0],Dx[1,1], '%s' % (\"X-1\"), size=20, zorder=1, color='k') \n",
        "ax.scatter(Dx[2,0], Dx[2,1], color='red')\n",
        "ax.text(Dx[2,0],Dx[2,1], '%s' % (\"X-2\"), size=20, zorder=1, color='k') \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZIn-TySDYcD1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Sabemos que la distancia euclidiana se define como la raiz cuadrada de la suma de las diferencias por dimensión al cuadrado:\n"
      ]
    },
    {
      "metadata": {
        "id": "SZMVo4owYcD2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def distancia_euclidiana(x, y):   \n",
        "    return np.sqrt(np.sum((x - y) ** 2))\n",
        "\n",
        "print(X[1:3,:])\n",
        "print('Distancia = ',distancia_euclidiana(X[1,:],X[2,:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-b7WtBl7YcD4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "De manera similar, la distancia manhattan se define como la suma de las diferencias por dimensión en valor absoluto:\n"
      ]
    },
    {
      "metadata": {
        "id": "iW70oQY9YcD5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def distancia_manhattan(x, y):   \n",
        "    return np.sum(np.absolute(x - y))\n",
        "\n",
        "print('Distancia = ',distancia_manhattan(X[1,:],X[2,:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FvHIpT3CYcD8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Ambas son casos particulares de la p-norma:\n"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "nBYSIT2AYcD9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def p_norma(x, y, p):   \n",
        "    return np.sum(np.absolute(x - y) ** p)#### TU CODIGO AQUI ####\n",
        "\n",
        "print('Diferencia entre 1-norma y dist. manhattan  = ',p_norma(X[1,:],X[2,:],1)-distancia_manhattan(X[1,:],X[2,:]))\n",
        "print('Diferencia entre 2-norma y dist. euclidiana = ',p_norma(X[1,:],X[2,:],2)-distancia_euclidiana(X[1,:],X[2,:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T8d51BBhYcEB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Otra generalización de la distancia euclidiana es la distancia de Mahalanobis, que tiene en cuenta la varianza de cada dimensión y las covarianzas entre ellas. Para poder calcular la distancia de Mahalanobis, en primer lugar se ha de estimar la matriz de covarianzas a partir de los datos:\n",
        "https://es.wikipedia.org/wiki/Matriz_de_covarianza\n",
        "\n",
        "En pocas palabras, se puede estimar, para cada par de dimensiones (a y b), como la suma del producto de cada valor de a menos su valor medio por el valor de b menos su valor medio, todo ello partido por el número de instancias menos 1.\n"
      ]
    },
    {
      "metadata": {
        "id": "vAhDr7jhYcEC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def matriz_covarianza(X):\n",
        "    Xm = X-np.mean(X,axis=0)\n",
        "    mC = np.zeros((X.shape[1],X.shape[1]))\n",
        "    for pair in it.product(np.arange(X.shape[1]), repeat=2):\n",
        "        mC[pair] = np.sum(#### TU CODIGO AQUI ####\n",
        "        )/float(X.shape[0] - 1)\n",
        "    return mC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ChW6EwbxYcEE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Una vez definida la matriz de covarianzas, la distancia de Mahalanobis se puede calcular, para dos instancias x e y, como la raiz cuadrada del producto del vector (x-y) transpuesto por la <b>inversa</b> de la matriz de covarianzas nuevamente por el vector (x-y):\n"
      ]
    },
    {
      "metadata": {
        "id": "zsHwG3RjYcEF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def distancia_mahalanobis(x,y,mCi):\n",
        "    v = x-y\n",
        "    v.shape=(v.size,1)\n",
        "    res = #### TU CODIGO AQUI ####\n",
        "    return res\n",
        "\n",
        "mC = matriz_covarianza(X)\n",
        "mCi = np.linalg.inv(mC)\n",
        "print('Distancia = ',distancia_mahalanobis(X[1,:],X[2,:],mCi), ' (** ESTA ES LA RESPUESTA A INCLUIR EN EL CAMPUS **)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kgzYVXXuYcEJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Como se comentaba en la clase de teoría, la distancia euclidiana y la de mahalanobis pueden dar resultados contradictorios. Veamos un ejemplo con el dataset que hemos cargado, Dx. En la imagen previa se pueden observar las posiciones de X0, X1 y X2. La instancia más cercana a X1 es X0 según la distancia euclidiana y X2 según la de Mahalanobis:\n"
      ]
    },
    {
      "metadata": {
        "id": "usfyf6-xYcEK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mdC = matriz_covarianza(Dx)\n",
        "mdCi = np.linalg.inv(mdC)\n",
        "print('Distancia X1-X0 mahalanobis = ',distancia_mahalanobis(Dx[1,:],Dx[0,:],mdCi), \n",
        "      '\\tvs. euclidiana = ', distancia_euclidiana(Dx[1,:],Dx[0,:]))\n",
        "print('Distancia X1-X2 mahalanobis = ',distancia_mahalanobis(Dx[1,:],Dx[2,:],mdCi), \n",
        "      '\\tvs. euclidiana = ', distancia_euclidiana(Dx[1,:],Dx[2,:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hMpbCBHUYcES",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "La similitud de coseno se define, por su parte, como el producto escalar de los dos vectores (instancias) dividido por el producto de las magnitudes de ambos vectores:\n"
      ]
    },
    {
      "metadata": {
        "id": "LtC63cZ7YcET",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def similitud_coseno(x, y):\n",
        "    return np.dot(x, y) / (np.sqrt(np.dot(x, x)) * #### TU CODIGO AQUI ####\n",
        "                          )\n",
        "\n",
        "print(similitud_coseno(X[0,:],X[1,:]))\n",
        "print(similitud_coseno(X[0,:],X[2,:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kF4NHuI_YcEX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Como ésta es una medida de similitud entre -1 y 1 (donde el valor 1 representa la misma dirección, o máxima similitud, y el valor -1 representa direcciones contrarias, o máxima disimilitud), si tomamos 1-similitud como medida de disimilitud no podemos hablar propiamente de distancia. Es más correcto referirnos a ella como disimilitud coseno:\n"
      ]
    },
    {
      "metadata": {
        "id": "HT3LE26hYcEY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def disimilitud_coseno(x, y):\n",
        "    return 1-similitud_coseno(x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2KD81PTpYcEb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Si todas las variables son binarias, podemos usar otro tipo de medidas, tales como:\n"
      ]
    },
    {
      "metadata": {
        "id": "Hb1wiRL5YcEc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def distancia_hamming(x, y):\n",
        "    return np.sum(x!=y)\n",
        "\n",
        "def similitud_jaccard(x, y):\n",
        "    return np.sum(x*y)/float(np.sum(x+y>0))\n",
        "\n",
        "def distancia_jaccard(x, y):\n",
        "    return 1-similitud_jaccard(x,y)\n",
        "\n",
        "Xb = np.random.randint(0,2,n_samples*5)\n",
        "Xb.shape = (n_samples, 5)\n",
        "\n",
        "print(distancia_hamming(Xb[1,:],Xb[2,:]))\n",
        "print(distancia_jaccard(Xb[1,:],Xb[2,:]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "glSEaW2xYcEf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Si entre nuestras variables existen variables discretas o categóricas, podemos usar una medida específica. En este caso, tendremos que combinarla con el resto mediante una distancia heterogénea:\n"
      ]
    },
    {
      "metadata": {
        "id": "nRDObf-8YcEg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# solo para una dimension\n",
        "def distancia_categorica(x, y):\n",
        "    return 1-int(x==y)\n",
        "\n",
        "def distancia_heterogenea(x, y, w, distancias):\n",
        "    return np.sum([w[d]*distancias[d](x[d],y[d]) for d in np.arange(x.size)])\n",
        "\n",
        "w = np.array([1,1,1,1,1])\n",
        "ds = [distancia_manhattan,distancia_manhattan, distancia_categorica,distancia_categorica,distancia_categorica]\n",
        "\n",
        "#distancia_heterogenea(Xb[1,:],Xb[2,:],w,ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dehbgl7HYcEi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "El vector de pesos w nos permite asignar diferente relevancia a las distintas dimensiones. Es posible calcular este vector de manera automática, siguiendo el método de Hastie et al. (2008):\n"
      ]
    },
    {
      "metadata": {
        "id": "JNfsYJd5YcEk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def calcular_pesos(X, distancias):\n",
        "    vW = np.zeros(X.shape[1])\n",
        "    for d in np.arange(X.shape[1]):\n",
        "        for pair in it.product(np.arange(X.shape[0]), repeat=2):\n",
        "            vW[d] += distancias[d](X[pair[0],d],X[pair[1],d])\n",
        "    vW /= float(X.shape[0]**2)\n",
        "    return 1/vW\n",
        "\n",
        "w_a = calcular_pesos(Xb, ds)\n",
        "\n",
        "print(w_a)\n",
        "\n",
        "print('Distancia heterogenea sin ajustar w  = ',distancia_heterogenea(Xb[1,:],Xb[2,:],w,ds))\n",
        "print('Distancia heterogenea con w ajustado = ',distancia_heterogenea(Xb[1,:],Xb[2,:],w_a,ds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WbqHW9wuYcEn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "<h2>Matriz de distancias</h2>\n",
        "\n",
        "Un concepto clave del aprendizaje no supervisado es el de <b>matriz de distancias</b>. Muchos algoritmos son capaces de funcionar directamente usando esta matriz en vez de la matriz-conjunto de datos de entrenamiento. Consiste en calcula la distancia entre todos los pares de instancias del conjunto de entrenamiento:\n"
      ]
    },
    {
      "metadata": {
        "id": "z_ysw7MWYcEo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def matriz_distancias(X, distancia):\n",
        "    mD = np.zeros((X.shape[0],X.shape[0]))\n",
        "    for pair in it.product(np.arange(X.shape[0]), repeat=2):\n",
        "        mD[pair] = distancia(#### TU CODIGO AQUI ####\n",
        "        )\n",
        "    return mD\n",
        "\n",
        "Ma = matriz_distancias(X, distancia_euclidiana)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JusVS5XtYcEs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "De manera equivalente, hablamos de matriz de similitudes cuando lo que se guarda en la matriz son valores de similitud (0, máxima diferencia; 1, máxima similitud):\n"
      ]
    },
    {
      "metadata": {
        "id": "1S3ON9I6YcEt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def matriz_similitud(X,distancia,C):\n",
        "    mD = matriz_distancias(X,distancia)\n",
        "    return np.exp(-mD**2/C)\n",
        "\n",
        "S = matriz_similitud(X, distancia_euclidiana,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uJfLEsgUYcEw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "<h2>Implementaciones en librerías de Python</h2>\n",
        "\n",
        "La mayoría de estas medidas ya están programadas en librerías de Python. Aquí tenemos unos ejemplos. Los comparamos con las que hemos programado para asegurarnos de que devuelven los mismos valores:\n"
      ]
    },
    {
      "metadata": {
        "id": "vDNtTLGNYcEx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import minkowski, mahalanobis, jaccard, hamming, euclidean, cosine\n",
        "\n",
        "# Calcular la matriz de covarianzas usando NumPy\n",
        "#np.cov(X.transpose())\n",
        "print('La diferencia entre las dos versiones de la matriz de covarianzas es = ',\n",
        "      np.round(np.sum(np.cov(X.transpose())-mC),10))\n",
        "\n",
        "# Calcular la distancia de Mahalanobis:\n",
        "#mahalanobis(X[1,:],X[2,:],mCi)\n",
        "print('La diferencia entre las dos versiones de la distancia de Mahalanobis es = ',\n",
        "      (distancia_mahalanobis(X[1,:],X[2,:],mCi)-mahalanobis(X[1,:],X[2,:],mCi)))\n",
        "\n",
        "# La función Minkowski de SciPy es la p-norma\n",
        "#minkowski(X[1,:],X[2,:],p)\n",
        "print('La diferencia entre las dos versiones de la p-norma (p=1) es = ',\n",
        "      (p_norma(X[1,:],X[2,:],1)-minkowski(X[1,:],X[2,:],1)),\n",
        "     ' o también (p=2) = ',(p_norma(X[1,:],X[2,:],2)-minkowski(X[1,:],X[2,:],2)))\n",
        "\n",
        "#Otras medidas son:\n",
        "#cosine(X[1,:],X[2,:])\n",
        "#euclidean(X[1,:],X[2,:])\n",
        "#jaccard(X[1,:],X[2,:])\n",
        "#hamming(X[1,:],X[2,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "miIh1vznYcEz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Ojo, ¡la función jaccard_similarity_score de la libraría sklearn no calcula el mismo valor!\n",
        "\n",
        "También existe la posibilidad de calcular la matriz de distancias directamente con la librería SciPy. Se le puede indicar el valor p de la p-norma a usar para calcular las distancias entre pares de instancias:\n"
      ]
    },
    {
      "metadata": {
        "id": "KGlTUH3ZYcE0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance_matrix\n",
        "\n",
        "Mb = distance_matrix(X, X, p=2)\n",
        "\n",
        "print('La diferencia entre las dos versiones de la matriz de covarianzas es = ', np.round(np.abs(np.sum(Ma-Mb)),10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tqHnqnBbYcE3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}